{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from utils.preprocessing import BasicPreprocessPipeline\n",
    "from scipy.stats import iqr\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from scipy.stats import norm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pd.set_option('display.max_columns', 50)",
   "id": "dbaf747cab114cc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_test = BasicPreprocessPipeline.run(pd.read_csv('data/sales_ads_test.csv'))\n",
    "df_train = BasicPreprocessPipeline.run(pd.read_csv('data/sales_ads_train.csv'))\n",
    "df_data = pd.concat((df_train, df_test), ignore_index=True, copy=True).reset_index()\n",
    "len(df_test), len(df_train)"
   ],
   "id": "b72bdeb22662b6fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Price investigation\n",
    "Analysing the distribution of price."
   ],
   "id": "1ecae66c63f079e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Distribution of the price\n",
    "plt.show(sns.displot(df_train.Cena))\n",
    "print(f\"Median : {df_train.Cena.quantile(0.5)}\")\n",
    "print(f\"Quantile 90% : {df_train.Cena.quantile(0.9)}\")\n",
    "print(f\"Top 10 : {df_train.Cena.sort_values(ascending=False).head(10).to_numpy()}\")"
   ],
   "id": "1e6de3d96cc0616e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "display(df_train.sort_values('Cena', ascending=False).head(2))  # The most severe ones\n",
    "print(f\"Average McLaren price : {df_train.groupby('Marka_pojazdu').agg({'Cena': 'mean'}).loc['McLaren', 'Cena']}\")"
   ],
   "id": "c7cc6ea7a35d2f6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_train.drop(df_train[df_train.Cena == 6999000.0].index, inplace=True)  # Such BMW seems like an anomaly\n",
    "df_data.drop(df_data[df_data.Cena == 6999000.0].index, inplace=True)  # Such BMW seems like an anomaly"
   ],
   "id": "59c01e83363fc3a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = df_train.Cena.to_numpy()\n",
    "x = np.log(x)\n",
    "x = (x - x.mean()) / (x.std())\n",
    "sns.displot(x, kde=True, stat=\"density\", bins=60)\n",
    "normal = np.linspace(min(x), max(x), 100)\n",
    "y = norm.pdf(normal, 0, 1)\n",
    "plt.plot(normal, y, 'r-', label='Normal Distribution')\n",
    "outliers = x[(x < -3) | (x > 3)]\n",
    "print(f\"Amount of outliers 3-sigma rule : {len(outliers)}\")\n",
    "print(f\"Outliers extremes : {df_train.Cena[(x < -3) | (x > 3)].min()}, {df_train.Cena[(x < -3) | (x > 3)].max()}\")\n",
    "print(f\"Outliers ~3 sigma range : {df_train.Cena.quantile(0.9975)} {df_train.Cena.quantile(0.0025)}\")\n",
    "print(f\"Outliers ~4 sigma range : {df_train.Cena.quantile(0.99975)} {df_train.Cena.quantile(0.00025)}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "a21be352edc0f29b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_train['Cena_log'] = df_train.Cena.apply(np.log)\n",
    "df_data['Cena_log'] = df_data.Cena.to_numpy()\n",
    "df_data.loc[~df_data['Cena_log'].isna(), 'Cena_log'] = df_train['Cena_log']"
   ],
   "id": "551d321b28dbf047",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notes are that data is very skewed to the left. There are a few extreme target outliers, \n",
    "which will be hard to tackle during prediction (probable premium car brands).\n",
    "Since top 2 cars are 3 times more expensive than the rest most expensive cars, they should\n",
    "be excluded. They are too big of an outlier. Without augmentation it would be extremely hard\n",
    "to take them into account. <br/><br/>\n",
    "Therefore, a log can be applied as data show patterns of exponential growth typical for sales data. After log\n",
    "transformation it can be seen that the data was in fact exponentially distributed. Furthermore, according to\n",
    "3-sigma rule we can identify values, which would be hard targets for the model as they fall into extreme\n",
    "regions of the normal distribution. This gives us total of 219 such samples."
   ],
   "id": "e142df5f202b168c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Horse power inverstigation\n",
    "Analysing the correlation between horsepower and other features. Tracking anomalies\n",
    "based on plots and logical real worlds assumptions (small engine can't have high power etc.)"
   ],
   "id": "85bcb68bf8b0c34e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.show(sns.displot(df_train.Moc_KM))\n",
    "print(f\"Top 10 horse power : {df_train.Moc_KM.sort_values(ascending=False).head(5).to_list()}\")\n",
    "print(f\"Top 10 horse power (Test) : {df_test.Moc_KM.sort_values(ascending=False).head(5).to_list()}\")\n",
    "print(f\"Top 10 horse power (Brands) : {df_train.sort_values('Moc_KM', ascending=False).Marka_pojazdu.head(5).to_list()}\")"
   ],
   "id": "d8fa441beac1c290",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "display(df_test.sort_values('Moc_KM', ascending=False).head(5))\n",
    "display(df_train.sort_values('Moc_KM', ascending=False).head(5))\n",
    "print(f\"IQR based range {df_test.Moc_KM.quantile(0.25) - 1.5 * iqr(df_test.Moc_KM.values, nan_policy='omit')} : {df_test.Moc_KM.quantile(0.75) + 1.5 * iqr(df_test.Moc_KM.values, nan_policy='omit')}\")\n",
    "sns.lmplot(x=\"Moc_KM\", y=\"Cena_log\", data=df_train, ci=None, line_kws={'color': 'red'})\n",
    "df_train.loc[:, 'Moc_KM_capped'] = df_train.Moc_KM.to_numpy()\n",
    "df_train.loc[df_train.Moc_KM > 800, 'Moc_KM_capped'] = 800\n",
    "plt.show(sns.displot(df_train.Moc_KM_capped))\n",
    "df_train.drop(columns='Moc_KM_capped', inplace=True)"
   ],
   "id": "42f00652f0bfb34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There are Suzuki and Passat with extreme horsepower values, which suggests some potential anomalies.\n",
    "To fix them the approach would be\n",
    "capping the value based on the standard deviation and mean of `Moc_KM` for each `Marka_pojazdu`. This could\n",
    "help as it is also applicable for the testing set.\n",
    "Furthermore, engine volume (`Pojemnocs_cm`) in real world has high correlation with power. Therefore, it could be used for\n",
    "detecting outliers. Here a simple linear regression line should discover some anomalies.\n",
    "<br/><br/>\n",
    "Anyway, it can be seen that correlation between price and power exists, but it is messy."
   ],
   "id": "6a07d9eb8a5cc70a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "value_threshold = 100\n",
    "value_second_threshold = 10\n",
    "sigma_scalar = 5\n",
    "print(f\"Value threshold : {value_threshold} - ({value_threshold / len(df_data) * 100:.5f}%)\")\n",
    "print(f\"Second value threshold : {value_second_threshold} - ({value_second_threshold / len(df_data) * 100:.5f}%)\")\n",
    "\n",
    "funcs = ('mean', 'std', lambda x: x.quantile(0.25), lambda x: x.quantile(0.5), lambda x: x.quantile(0.75))\n",
    "power_table = ((df_data.groupby('Marka_pojazdu').agg({'Moc_KM': funcs}))\n",
    "               .sort_values(('Moc_KM', 'mean'), ascending=False))\n",
    "power_table.columns = ('mean', 'std', 'Q1', 'Q2', 'Q3')\n",
    "\n",
    "value_counts = df_data.Marka_pojazdu.value_counts()[power_table.index]\n",
    "plt.figure(figsize=(14, 6))\n",
    "common_brands = power_table[value_counts >= value_threshold]\n",
    "plt.bar(common_brands.index, common_brands.loc[:, 'mean'])\n",
    "plt.title('Power based on car brand (Medium Tier Segment)')\n",
    "plt.show()\n",
    "\n",
    "ceiling = common_brands.loc[:, 'mean'] + common_brands.loc[:, 'std'] * sigma_scalar\n",
    "def cap_power_based_on_brand(row: pd.Series):\n",
    "    row = row.copy()\n",
    "    if row.Marka_pojazdu not in common_brands.index:\n",
    "        return row\n",
    "    if row.Moc_KM > ceiling[row.Marka_pojazdu]:\n",
    "        row.Moc_KM = ceiling[row.Marka_pojazdu]\n",
    "    return row\n",
    "df_data_capped = df_data.loc[:, ['Moc_KM', 'Marka_pojazdu', 'Cena_log']].apply(cap_power_based_on_brand, axis=1)\n",
    "\n",
    "uncommon_brands = power_table[(value_counts < value_threshold) & (value_counts >= value_second_threshold)]\n",
    "IQR = uncommon_brands.loc[:, 'Q3'] - uncommon_brands.loc[:, 'Q1']\n",
    "ceiling = uncommon_brands.loc[:, 'Q2'] + 1.5 * IQR\n",
    "floor = uncommon_brands.loc[:, 'Q2'] - 1.5 * IQR\n",
    "def cap_power_based_on_brand(row: pd.Series):\n",
    "    row = row.copy()\n",
    "    if row.Marka_pojazdu not in uncommon_brands.index:\n",
    "        return row\n",
    "    if row.Moc_KM > ceiling[row.Marka_pojazdu]:\n",
    "        row.Moc_KM = ceiling[row.Marka_pojazdu]\n",
    "    elif row.Moc_KM < floor[row.Marka_pojazdu]:\n",
    "        row.Moc_KM = floor[row.Marka_pojazdu]\n",
    "    return row\n",
    "df_data_capped = df_data_capped.apply(cap_power_based_on_brand, axis=1)\n",
    "sns.lmplot(x=\"Moc_KM\", y=\"Cena_log\", data=df_data_capped, ci=None, line_kws={'color': 'red'})\n",
    "\n",
    "power_table = df_data_capped.groupby('Marka_pojazdu').agg({'Moc_KM': ('mean', 'std')}).sort_values(('Moc_KM', 'mean'), ascending=False)\n",
    "value_counts = df_data_capped.Marka_pojazdu.value_counts()[power_table.index]\n",
    "plt.figure(figsize=(14, 6))\n",
    "common_brands = power_table[value_counts >= value_threshold]\n",
    "plt.bar(common_brands.index, common_brands.loc[:, ('Moc_KM', 'mean')])\n",
    "plt.title('Power based on car brand (Medium Tier Segment)')\n",
    "plt.show()\n",
    "\n",
    "df_data.loc[:, 'Moc_KM_brand_capped'] = df_data_capped.Moc_KM.to_numpy()\n",
    "total_capped = (df_data.Moc_KM != df_data.Moc_KM_brand_capped)[~df_data.Moc_KM.isna()].sum()\n",
    "print(f\"Capped total of {total_capped} samples, which is {total_capped / len(df_data) * 100:.4f}%\")"
   ],
   "id": "1654bf844abac04e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here values were clipped based on the average horsepower per brand category. Categories with high amount of\n",
    "samples were treated with 3-sigma rule to take into account variance of the data, whereas small samples\n",
    "were filtered by IQR due to unreliable standard deviation."
   ],
   "id": "cb8335d3b3ddb205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Manual checking for potential problems and ideas\n",
    "df_data_capped.sort_values('Moc_KM', ascending=False).head(2)\n",
    "display(df_data_capped.sort_values('Moc_KM', ascending=False).head(15))"
   ],
   "id": "323a214f416a6df0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Additionally, the engine volume can be taken into account. Notice, that although high volume engines can have\n",
    "low power due to low rpm limit, inefficient burning and so on, the small volume engines are very hard to\n",
    "improve on power. Therefore, the outliers below regression line should be taken into account.<br/><br/>\n",
    "Keep in mind the volume may also have outliers! Furthermore, in this case horsepower outliers were managed\n",
    "by the categories (with logical assumptions that categories has strong influence on the horsepower). Thus,\n",
    "it is more probable that the engine volume contains outliers rather than horsepower."
   ],
   "id": "1a73b74deb2d2ecf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.lmplot(x=\"Moc_KM\", y=\"Pojemnosc_cm3\", data=df_data, ci=None, line_kws={'color': 'red'})\n",
    "plt.title(\"Correlation horsepower and engine volume (Train + Test)\")\n",
    "plt.show()\n",
    "sns.lmplot(x=\"Moc_KM_brand_capped\", y=\"Pojemnosc_cm3\", data=df_data, ci=None, line_kws={'color': 'red'})\n",
    "plt.title(\"Correlation horsepower:fixed and engine volume (Train + Test)\")\n",
    "plt.show()"
   ],
   "id": "9bd0a8d7baaf1659",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Funny horizontal line patterns on the plot are due to often engines fixed volume, which can have\n",
    "various power based on compression, turbo etc."
   ],
   "id": "1ab3deaa4cb82bf7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "error_threshold = -2_001  # Only for significant outliers\n",
    "volume_threshold = 1_001  # Only for engines smaller than 1 liter (assumption based on car domain knowledge)\n",
    "volume_min = 1_000\n",
    "decay_scalar = 0.5  # How much to diminish the total outliers\n",
    "mask = ~(df_data.Pojemnosc_cm3.isna() | df_data.Moc_KM_brand_capped.isna())\n",
    "slope, bias = np.polyfit(df_data.Moc_KM_brand_capped[mask].to_numpy(),\n",
    "                         df_data.Pojemnosc_cm3[mask].to_numpy(), 1)\n",
    "errors = (df_data.Pojemnosc_cm3[mask].to_numpy() - (bias + slope * df_data.Moc_KM_brand_capped[mask].to_numpy()))\n",
    "args = np.argwhere((df_data.Pojemnosc_cm3[mask].to_numpy() < volume_threshold) & (errors < error_threshold))\n",
    "indices = df_data.Moc_KM_brand_capped[mask].index[args.flatten()]\n",
    "\n",
    "df_data.loc[:, 'Pojemnosc_cm3_capped'] = df_data.Pojemnosc_cm3.to_numpy()\n",
    "df_data.loc[indices, 'Pojemnosc_cm3_capped'] = (df_data.loc[indices, 'Pojemnosc_cm3_capped'] + bias + slope * df_data.Moc_KM_brand_capped[mask].to_numpy()[indices]) / 2\n",
    "sns.lmplot(x=\"Moc_KM_brand_capped\", y=\"Pojemnosc_cm3_capped\", data=df_data, ci=None, line_kws={'color': 'red'})\n",
    "plt.title(\"Correlation horsepower:fixed and engine volume (Train + Test)\")\n",
    "plt.show()\n",
    "df_data.loc[indices]"
   ],
   "id": "b9e3b482cb5b4c90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sns.lmplot(x=\"Pojemnosc_cm3\", y=\"Cena_log\", data=df_train, ci=None, line_kws={'color': 'red'})",
   "id": "753e310b7956d10f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There are only a few extreme outliers in this small engine volume region. Nontheless, for future more robust\n",
    "model one fact should be remembered. The electric cars like Tesla shoud have their engine volume deleted or\n",
    "be marked as electric."
   ],
   "id": "56cd6b4d7a1d3c44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Top 10 mileage : {df_data.Przebieg_km.sort_values(ascending=False).head(5).to_list()}\")\n",
    "print(f\"Top 10 mileage (Test): {df_data.Przebieg_km.sort_values(ascending=False).head(5).to_list()}\")\n",
    "print(f\"Top 10 mileage : {df_data.sort_values('Przebieg_km', ascending=False).Przebieg_km.head(5).to_list()}\")\n",
    "print(f\"Quantile 90% mileage : {df_data.Przebieg_km.quantile(0.9)}\")\n",
    "print(f\"IQR based range {df_data.Przebieg_km.quantile(0.25) - 1.5 * iqr(df_data.Przebieg_km.to_numpy(), nan_policy='omit')} : {df_data.Przebieg_km.quantile(0.75) + 1.5 * iqr(df_data.Przebieg_km.to_numpy(), nan_policy='omit')}\")\n",
    "max_value = df_data.Przebieg_km.quantile(0.75) + 1.5 * iqr(df_data.Przebieg_km.to_numpy(), nan_policy='omit')\n",
    "super_max_value = df_data.Przebieg_km.quantile(0.75) + 5.0 * iqr(df_data.Przebieg_km.to_numpy(), nan_policy='omit')\n",
    "print(f\"Amount of IQR outliers : {df_data[df_data.Przebieg_km > max_value].shape[0]}\")\n",
    "sns.lmplot(x=\"Przebieg_km\", y=\"Cena\", data=df_data, ci=None)  # Ci will look ridiculous here\n",
    "\n",
    "\n",
    "df_data.loc[:, 'Przebieg_km_capped'] = df_data.Przebieg_km.to_numpy()\n",
    "df_data.loc[df_data.Przebieg_km > max_value, 'Przebieg_km_capped'] = max_value\n",
    "df_data.loc[:, 'Przebieg_km_cleared'] = df_data.Przebieg_km.to_numpy()\n",
    "df_data.loc[df_data.Przebieg_km > super_max_value, 'Przebieg_km_cleared'] = np.nan\n",
    "sns.lmplot(x=\"Przebieg_km_capped\", y=\"Cena\", data=df_data, ci=None, line_kws={'color': 'red'})\n",
    "sns.lmplot(x=\"Przebieg_km_capped\", y=\"Cena_log\", data=df_data, ci=None, line_kws={'color': 'red'})\n",
    "print(f\"Duplicates after clipping : {(df_data.Przebieg_km_capped == max_value).sum()}\")\n",
    "plt.show()"
   ],
   "id": "68dc10dc3f46a1a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.histplot(x=\"Przebieg_km_capped\", data=df_data, bins=100)\n",
    "plt.show()"
   ],
   "id": "e8516f465306345e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here, on the other hand cutting the mileage based on the IQR might work. It can be seen that outliers are so big\n",
    "that is hard to see the correlation. After capping the value correlation is much clearer and it can be seen\n",
    "that for larger values of mileage it is decent.<br/></br>\n",
    "Nonetheless, it will not help with small values."
   ],
   "id": "7997a7622f4bafeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.displot(df_data.Rok_produkcji)\n",
    "plt.title(\"Year of production distribution (Train)\")\n",
    "plt.show()\n",
    "print(f\"Top 10 age : {df_data.Rok_produkcji.sort_values().head(5).to_list()}\")\n",
    "print(\"Oldest 10 age brands : \")\n",
    "display(df_data.sort_values('Rok_produkcji').head(5))\n",
    "\n",
    "sns.lmplot(x=\"Rok_produkcji\", y=\"Cena_log\", data=df_train, ci=None, line_kws={'color': 'red'})\n",
    "sns.lmplot(x=\"Rok_produkcji\", y=\"Przebieg_km_cleared\", data=df_data, ci=None, line_kws={'color': 'red'})"
   ],
   "id": "fe8cec94581c2238",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imputation for numerical features",
   "id": "b0485248853eddd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "power_brand_sum = (df_data.Moc_KM.isna() & df_data.Marka_pojazdu.notna()).sum()\n",
    "power_model_brand_sum = (df_data.Moc_KM.isna() & df_data.Model_pojazdu.isna() & df_data.Marka_pojazdu.notna()).sum()\n",
    "power_brand_sum, power_model_brand_sum"
   ],
   "id": "9cab7da94db63724",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Low ratio above indicates plethora of Horse Power missing data could be filled based on the brand value.",
   "id": "1a330650a3f07b8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(100)\n",
    "mask = df_data.Moc_KM.isna() & df_data.Marka_pojazdu.notna()\n",
    "power = df_data.groupby('Marka_pojazdu').agg({'Moc_KM_brand_capped': ('mean', 'std')})\n",
    "counts = df_data.Marka_pojazdu.value_counts()\n",
    "reliable_references = counts.index[counts > 30]\n",
    "\n",
    "display(df_data.loc[mask, ('Moc_KM_brand_capped', 'Marka_pojazdu')].head(5))\n",
    "mask = df_data.Marka_pojazdu.isin(reliable_references) & mask\n",
    "df_data.loc[:, 'Moc_KM_filled'] = df_data.Moc_KM_brand_capped.to_numpy()\n",
    "df_data.loc[mask, 'Moc_KM_filled'] = df_data.loc[mask, 'Marka_pojazdu'].apply(\n",
    "    lambda row: (power.loc[row, ('Moc_KM_brand_capped', 'mean')] +\n",
    "                 np.random.uniform(-0.2, 0.2) * power.loc[row, ('Moc_KM_brand_capped', 'std')])\n",
    ")\n",
    "df_data.loc[mask, ('Moc_KM_filled', 'Marka_pojazdu')].head(5)"
   ],
   "id": "371a84dac2f5c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Too extreme outliers make less reliable validation\n",
    "df_data.drop(index=df_data.index[df_data.Cena > 6_000_000], inplace=True)"
   ],
   "id": "be3a8c0ef140231",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_data.columns",
   "id": "92ac4a33821ffbba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from typing import Any, List\n",
    "\n",
    "##### For category wrapping or omitting\n",
    "def cat_wrap(df, column, threshold):\n",
    "    counts = df[column].value_counts()\n",
    "    mask = df[column].isin(counts[counts < threshold].index)\n",
    "    df.loc[mask, column] = 'Other'\n",
    "    return df\n",
    "\n",
    "def cat_select(df, column, threshold):\n",
    "    counts = df[column].value_counts()\n",
    "    mask = df[column].isin(counts[counts < threshold].index)\n",
    "    df.loc[mask, column] = np.nan\n",
    "    return df\n",
    "\n",
    "def target_encode(df: pd.DataFrame, df_unknowns: Any, features: List):\n",
    "    ##### Target Encoding for high cardinality\n",
    "    for cat in tqdm(cat_features_high_card):\n",
    "        df = cat_select(df, cat, 10)\n",
    "        engineered_stats = ['median', 'max', 'min', 'nunique']\n",
    "        table_of_cat_stats = df.groupby(cat).agg({'Cena': engineered_stats})\n",
    "        engineered_features = []\n",
    "        for stat in engineered_stats:\n",
    "            stat_series = table_of_cat_stats[('Cena', stat)]\n",
    "            df.loc[:, f'{cat}_{stat}'] = df.loc[:, cat].map(stat_series)\n",
    "            if df_unknowns is not None:\n",
    "                if isinstance(df_unknowns, pd.DataFrame):\n",
    "                    df_unknowns.loc[:, f'{cat}_{stat}'] = df_unknowns.loc[:, cat].map(stat_series)\n",
    "                elif isinstance(df_unknowns, list):\n",
    "                    for dfu in df_unknowns:\n",
    "                        dfu.loc[:, f'{cat}_{stat}'] = dfu.loc[:, cat].map(stat_series)\n",
    "                else:\n",
    "                    raise ValueError('Expected unknown data frames to be list or a single data frame.')\n",
    "            engineered_features.append(f'{cat}_{stat}')\n",
    "        features += engineered_features\n",
    "    \n",
    "    #### Either One Hot encoding or different Target encoding\n",
    "    for cat in tqdm(cat_features_low_card):\n",
    "        df = cat_select(df, cat, 50)\n",
    "        engineered_stats = ['median', 'mean', 'std']\n",
    "        table_of_cat_stats = df.groupby(cat).agg({'Cena': engineered_stats})\n",
    "        engineered_features = []\n",
    "        for stat in engineered_stats:\n",
    "            stat_series = table_of_cat_stats[('Cena', stat)]\n",
    "            df.loc[:, f'{cat}_{stat}'] = df.loc[:, cat].map(stat_series)\n",
    "            if df_unknowns is not None:\n",
    "                if isinstance(df_unknowns, pd.DataFrame):\n",
    "                    df_unknowns.loc[:, f'{cat}_{stat}'] = df_unknowns.loc[:, cat].map(stat_series)\n",
    "                elif isinstance(df_unknowns, list):\n",
    "                    for dfu in df_unknowns:\n",
    "                        dfu.loc[:, f'{cat}_{stat}'] = dfu.loc[:, cat].map(stat_series)\n",
    "                else:\n",
    "                    raise ValueError('Expected unknown data frames to be list or a single data frame.')\n",
    "            engineered_features.append(f'{cat}_{stat}')\n",
    "        features += engineered_features\n",
    "    return df, df_unknowns\n",
    "\n",
    "# Selected features\n",
    "cat_features_high_card = ['Marka_pojazdu', 'Model_pojazdu', 'Generacja_pojazdu']\n",
    "cat_features_low_card = ['Naped', 'Skrzynia_biegow', 'Typ_nadwozia', 'Kolor', 'Liczba_drzwi',\n",
    "                         'Kraj_pochodzenia']\n",
    "num_features=['Przebieg_km_cleared', 'Moc_KM_brand_capped', 'Pojemnosc_cm3_capped', 'Rok_produkcji']\n",
    "features = num_features\n",
    "\n",
    "# Encode the targets for the\n",
    "assert np.all(df_test.ID.to_numpy() == df_data.loc[df_data.Cena.isna(), 'ID'].to_numpy()), \"General data frame is incorrectly merged.\"\n",
    "# Fill the missing brands with their equivalents\n",
    "print(\"Maybach is chainged into Rolls-Royce for testing as it does not appear in training.\")\n",
    "df_test.loc[df_test.Marka_pojazdu == 'Maybach', 'Marka_pojazdu'] = 'Rolls-Royce'\n",
    "print(\"Brands, which are in test, but not train. They will be set to Nan.\")\n",
    "print(df_test.loc[~np.isin(df_test.Marka_pojazdu, df_train.Marka_pojazdu.unique()), 'Marka_pojazdu'].to_numpy())\n",
    "df_test.loc[~np.isin(df_test.Marka_pojazdu, df_train.Marka_pojazdu.unique()), 'Marka_pojazdu'] = np.nan\n",
    "\n",
    "df = df_data.loc[~df_data.Cena.isna()].copy(deep=True).reset_index()\n",
    "df_submition = df_data.loc[df_data.Cena.isna()].copy(deep=True).reset_index()"
   ],
   "id": "5885f70f6d33abc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from copy import deepcopy\n",
    "\n",
    "##### Split the data into K folds, but preserve price distribution\n",
    "num_folds = 6\n",
    "target_column='Cena'\n",
    "df.sort_values(by=target_column, inplace=True, kind='stable')  # Split randomly using modulo, but having even price distribution\n",
    "folds_idx = np.arange(len(df))\n",
    "for i in folds_idx:\n",
    "    folds_idx[i] = i % num_folds\n",
    "folds = []\n",
    "for i in range(num_folds):\n",
    "    test_mask = folds_idx == i % num_folds\n",
    "    folds.append((np.argwhere(~test_mask).flatten(), np.argwhere(test_mask).flatten()))\n",
    "y = df[target_column]\n",
    "\n",
    "# XGBoost parameters\n",
    "params = {\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.01,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,  # Equivalent to rsm\n",
    "    'min_child_weight': 10,     # Smooth the groups, don't allow luxury cars to overfit\n",
    "    'random_state': 100,\n",
    "    'reg_lambda': 10,         # L2 regularization (similar to l2_leaf_reg)\n",
    "    'objective': 'reg:squarederror',  # 'pseudohubererror'\n",
    "    'eval_metric': 'rmse',\n",
    "}\n",
    "\n",
    "# Cross-validation stats\n",
    "cv_results = {\n",
    "    'test-rmse-mean': [],\n",
    "    'test-rmse-std': [],\n",
    "    'train-rmse-mean': [],\n",
    "    'train-rmse-std': []\n",
    "}\n",
    "\n",
    "fold_scores = []\n",
    "hardest_examples = []\n",
    "best_model = None\n",
    "best_rmse = 1_000_000\n",
    "safe_num_of_estimators = 0\n",
    "for i, (train_idx, test_idx) in enumerate(folds):\n",
    "    print(f\"Fold {i+1}/{num_folds}\")\n",
    "\n",
    "    X_train, y_train = df.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_test, y_test = df.iloc[test_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Encode the categories\n",
    "    # and don't expose data information from validation to train (No data leakage)\n",
    "    all_features = deepcopy(features)\n",
    "    X_train, X_test = target_encode(X_train.copy(deep=True), X_test.copy(deep=True), all_features)\n",
    "    X_train, X_test = X_train[all_features], X_test[all_features]\n",
    "\n",
    "    # Create DMatrix objects for XGBoost\n",
    "    dtrain = xgb.DMatrix(X_train.to_numpy(), label=y_train, feature_names=all_features)\n",
    "    dtest = xgb.DMatrix(X_test.to_numpy(), label=y_test, feature_names=all_features)\n",
    "\n",
    "    # Train the model\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=15_000,\n",
    "        evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "\n",
    "    # Get predictions\n",
    "    train_preds = model.predict(dtrain)\n",
    "    test_preds = model.predict(dtest)\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_rmse = sqrt(mean_squared_error(y_train, train_preds))\n",
    "    test_rmse = sqrt(mean_squared_error(y_test, test_preds))\n",
    "    \n",
    "    # Save model if it has the lowest RMSE\n",
    "    if test_rmse < best_rmse:\n",
    "        best_rmse = test_rmse\n",
    "        best_model = model\n",
    "    safe_num_of_estimators = (safe_num_of_estimators * i + model.best_iteration) / (i + 1)\n",
    "    \n",
    "    # Get the examples, which were the hardest to classify\n",
    "    test_mse = (y_test - test_preds) ** 2\n",
    "    hardest_args = np.argsort(test_mse)[-5:]\n",
    "    hardest_validation_examples = X_test.iloc[hardest_args].copy(deep=True)\n",
    "    hardest_validation_examples.loc[:, 'Prediction'] = test_preds[hardest_args]\n",
    "    hardest_validation_examples.loc[:, 'Cena'] = y_test.iloc[hardest_args]\n",
    "    hardest_examples.append(hardest_validation_examples)\n",
    "\n",
    "    print(f\"Fold {i+1} - Train RMSE: {train_rmse:.6f}, Test RMSE: {test_rmse:.6f}\")\n",
    "    fold_scores.append((train_rmse, test_rmse))\n",
    "\n",
    "# Stats on best model\n",
    "print(f\"Best validation RMSE: {best_rmse:.6f}\")\n",
    "print(f\"Average number of estimators: {safe_num_of_estimators}\")\n",
    "\n",
    "# Summarize results\n",
    "train_scores = [score[0] for score in fold_scores]\n",
    "test_scores = [score[1] for score in fold_scores]\n",
    "\n",
    "cv_results['train-rmse-mean'] = np.mean(train_scores)\n",
    "cv_results['train-rmse-std'] = np.std(train_scores)\n",
    "cv_results['test-rmse-mean'] = np.mean(test_scores)\n",
    "cv_results['test-rmse-std'] = np.std(test_scores)\n",
    "\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Train RMSE: {cv_results['train-rmse-mean']:.6f} ± {cv_results['train-rmse-std']:.6f}\")\n",
    "print(f\"Test RMSE: {cv_results['test-rmse-mean']:.6f} ± {cv_results['test-rmse-std']:.6f}\")"
   ],
   "id": "4c0beaeb6db14637",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "xgb.plot_importance(\n",
    "    model,\n",
    "    ax=ax,\n",
    "    max_num_features=25,\n",
    "    importance_type=\"weight\", # 'gain', 'cover', 'total_gain', 'total_cover'\n",
    ")\n",
    "plt.title(\"XGBoost Top Features\")\n",
    "plt.show()"
   ],
   "id": "27cee175da94b584",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train the final model\n",
    "Traininig the final model is performed on the whole dataset to maximise the gain from the data. In such case one have\n",
    "to be extra careful about overfitting."
   ],
   "id": "616b2b7548275549"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create very small validation set for final training early stopping\n",
    "modulo = 1000\n",
    "df.sort_values(by=target_column, inplace=True, kind='stable')  # Split randomly using modulo, but having even price distribution\n",
    "\n",
    "test_mask = np.arange(len(df)) % modulo == 0\n",
    "train_idx, test_idx = np.argwhere(~test_mask).flatten(), np.argwhere(test_mask).flatten()\n",
    "\n",
    "y = df[target_column]\n",
    "\n",
    "X_train, y_train = df.iloc[train_idx], y.iloc[train_idx]\n",
    "X_test, y_test = df.iloc[test_idx], y.iloc[test_idx]\n",
    "\n",
    "# Encode the categories\n",
    "# and don't expose data information from validation to train (No data leakage)\n",
    "all_features = deepcopy(features)\n",
    "X_train, (X_test, X_submition) = target_encode(X_train.copy(deep=True), [X_test.copy(deep=True), df_submition.copy(deep=True)], all_features)\n",
    "X_train, X_test, X_submition = X_train.loc[:, all_features], X_test.loc[:, all_features], X_submition.loc[:, all_features]\n",
    "\n",
    "# Create DMatrix objects for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train.to_numpy(), label=y_train, feature_names=all_features)\n",
    "dtest = xgb.DMatrix(X_test.to_numpy(), label=y_test, feature_names=all_features)\n",
    "# Train the model\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=900,     # To ensure there is no overfitting (based on where model improvement plateaus)\n",
    "    evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=100\n",
    ")\n",
    "\n",
    "# Predict\n",
    "dsub = xgb.DMatrix(X_submition.to_numpy(), feature_names=all_features)\n",
    "predictions = model.predict(dsub)\n",
    "print(f\"Maximal prediction value detected : {predictions.max()}\")\n",
    "print(f\"Maximal target in train : {y_train.max()}\")\n",
    "print(f\"Maximal target in test : {y_test.max()}\")"
   ],
   "id": "9bc610a8203bef01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.preprocessing import preprocess_currency\n",
    "\n",
    "model.save_model('submission_model.json')\n",
    "\n",
    "df_submit = df_test.copy(deep=True)\n",
    "df_submit['Cena'] = predictions\n",
    "df_submit = preprocess_currency(df_submit, invert=True)\n",
    "\n",
    "df_submit.drop(columns=[col for col in df_test.columns if col not in ['ID', 'Cena']], inplace=True)\n",
    "display(df_submit)\n",
    "df_submit.to_csv('submission.csv', index=False)"
   ],
   "id": "cd3a92e19a988475",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_test.iloc[np.argsort(predictions)[-2]]",
   "id": "eba2b64c031e5bdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.sort(predictions)[-1]",
   "id": "57d4546428e3fcb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_submit = df_test.copy(deep=True)\n",
    "df_submit['Cena'] = predictions\n",
    "df_submit = preprocess_currency(df_submit, invert=True)\n",
    "\n",
    "mask = df_submit.Waluta == 'EUR'\n",
    "print(predictions[mask][:10], df_submit.Cena[mask].to_numpy()[:10])"
   ],
   "id": "e0209e951bc63188",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "predictions[1]",
   "id": "3535fc37a5a1f41f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f9f205870b7f4578",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
